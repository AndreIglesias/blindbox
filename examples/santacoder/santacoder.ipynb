{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7-tQfVM_pJcr"
   },
   "source": [
    "# Santacoder using FastAPI\n",
    "______________________________\n",
    "\n",
    "## Introduction\n",
    "______________________________\n",
    "\n",
    "In this tutorial, we're going to walk through how we created an application image for deploying the [Santacoder LLM model](https://huggingface.co/bigcode/santacoder) with BlindBox. The Santacoder model performs code generation, it fills in code when given the start of a code block. By deploying Santacoder with BlindBox, SaaS owners can be sure the code they sent the model is kept confidential at all times and is not exposed to the service provider. \n",
    "\n",
    "We will cover two key steps in this tutorial:\n",
    "\n",
    "1. How we **created a BlindBox-compatible API** for the model.\n",
    "2. How we **created the Docker image** for our API.\n",
    "\n",
    "> You can see how we deploy the image with BlindBox in the [Quick tour](https://blindbox.mithrilsecurity.io/en/latest/docs/getting-started/quick-tour/).\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "____________________\n",
    "\n",
    "To follow along with this tutorial, you will need to:\n",
    "\n",
    "+ Have Docker installed in your environment. Here's the [Docker installation guide](https://docs.docker.com/desktop/install/linux-install/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging Santacoder with FastAPI\n",
    "_______________________\n",
    "\n",
    "Our first task in deploying the **Whisper OpenAI** model with **BlindBox** was to create an API so that our end users will be able to query the model. We did this using the **FastAPI library** which allows us to quickly assign functions to API endpoints.\n",
    " \n",
    "The full code we use to do this is available in the `server.py` file in the `examples/santacoder` folder on BlindBox's official GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mithril-security/blindbox\n",
    "!cd examples/santacoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key sections in this code:\n",
    "\n",
    "### Initial set-up\n",
    "\n",
    "Firstly, we load the santacoder model from Hugging Face, and initialize our API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    " # specify HuggingFace model name\n",
    "model_name = \"bigcode/santacoder\"\n",
    "\n",
    " # get tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    " # for GPU usage or cpu for CPU usage\n",
    "device = \"cuda\"\n",
    "\n",
    " # get model and call eval\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).eval()\n",
    "\n",
    " # optional: optimize for cpu inference\n",
    "model = ipex.optimize(model)\n",
    "\n",
    " # initalize our API object\n",
    "app = FastAPI()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an endpoint\n",
    "\n",
    "Secondly, we create a `/generate` POST endpoint on our FastAPI application object. The user will be able to send their prompt to this endpoint and get back the model's generated code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class GenerateRequest(BaseModel):\n",
    "    # user input\n",
    "    input_text: str\n",
    "    # set token limit\n",
    "    max_new_tokens: int = 128\n",
    "\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "def generate(req: GenerateRequest):\n",
    "\n",
    "    # prepare input for model\n",
    "    inputs = tokenizer(req.input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # run santacoder on inputs and get returned response\n",
    "    outputs = model.generate(inputs)\n",
    "\n",
    "    # convert output to string\n",
    "    text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    # return text output\n",
    "    return {\"text\": text}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching our server\n",
    "\n",
    "Finally, we deploys our API on a python ASGI `uvicorn` server (an asynchronous web server) on `port 80`. It is essential to use **port 80** as BlindBox will need to be able to communicate with our application on this port!\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=80)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, we packaged the Whisper model as an API by doing the following:\n",
    "\n",
    "+ Creating an API app object that \"configures\" the `uvicorn` server by providing handlers for specific endpoints\n",
    "\n",
    "+ Creating a `generate` endpoint which in turn queries the santacoder model.\n",
    "\n",
    "+ Deploy our API on our server on `port 80`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging our application in a Docker image\n",
    "________________________________\n",
    "\n",
    "Once we had created out Whisper API, all that was left to do was create a **Docker image** for our application that could then be deployed in BlindBox. Let's take a look at the Dockerfile we used to do this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```docker\n",
    "FROM python:3.10.10-bullseye as base\n",
    "\n",
    "# install dependencies\n",
    "RUN pip install \\\n",
    "    torch==1.13.1 \\\n",
    "    transformers==4.26.1 \\\n",
    "    fastapi==0.95.0 \\\n",
    "    python-multipart==0.0.6 \\\n",
    "    uvicorn==0.21.1 \\\n",
    "    pydantic==1.10.7 \\\n",
    "    intel_extension_for_pytorch==1.13.100 \\\n",
    "    --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# copy our app code to container\n",
    "COPY ./server.py ./\n",
    "\n",
    "# signal that our application runs on port 80\n",
    "EXPOSE 80\n",
    "\n",
    "# run our app server\n",
    "CMD python ./server.py\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Same as for the application code, this file can be viewed in the `examples/santacoder` folder on the official BlindBox GitHub repository.\n",
    "\n",
    "There are no complex requirements for the Docker image, but it is recommended to `EXPOSE` port 80 to signal that the application will be running on port 80 within our BlindBox."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "_______________________\n",
    "\n",
    "The Santacoder app is now ready to be built and deployed on BlindBox! You can see exactly how we do this in our [Quick Tour](https://blindbox.mithrilsecurity.io/en/latest/docs/getting-started/quick-tour/).\n",
    " \n",
    "In this tutorial, we've seen how we can:\n",
    "+ Create a **BlindBox-compatible application**\n",
    "+ Create an **application image**, ready to be built and deployed on BlindBox!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "blindbox-preview-7Yaoi9am-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
