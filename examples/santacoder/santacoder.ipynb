{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7-tQfVM_pJcr"
   },
   "source": [
    "# Santacoder using FastAPI\n",
    "______________________________\n",
    "\n",
    "## Introduction\n",
    "______________________________\n",
    "\n",
    "In this tutorial, we're going to do a walk through of how we created an API for the [Santacoder LLM model](https://huggingface.co/bigcode/santacoder) and deployed it with with BlindBox. The Santacoder model performs code generation. When given the start of a code block. it will autocomplete the rest of the block. By deploying Santacoder with BlindBox, developers working with private code bases can be sure the code they send to the model is kept confidential at all times and is not exposed to the service provider. \n",
    "\n",
    "Preparing and deploying an application with BlindBox can be broken down into four stages:\n",
    "\n",
    "[IMAGE HERE]\n",
    "\n",
    "In this tutorial, we are primarily going to focus on the first step: Preparing the application image.\n",
    "\n",
    "We will split this into two sub-steps:\n",
    "\n",
    "1. How we **created a BlindBox-compatible API** for the model.\n",
    "2. How we **created the Docker image** for our API.\n",
    "\n",
    "Once we have covered this step in detail, we will provide a quick demonstration of how to deploy and query the application image.\n",
    "\n",
    "> You can see how we deploy the image with BlindBox in the [Quick tour](https://blindbox.mithrilsecurity.io/en/latest/docs/getting-started/quick-tour/).\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "____________________\n",
    "\n",
    "To follow along with this tutorial, you will need to:\n",
    "\n",
    "+ Have Docker installed in your environment. Here's the [Docker installation guide](https://docs.docker.com/desktop/install/linux-install/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging Santacoder with FastAPI\n",
    "_______________________\n",
    "\n",
    "Our first task in deploying the **Whisper OpenAI** model with **BlindBox** was to create an API so that our end users will be able to query the model. We did this using the **FastAPI library** which allows us to quickly assign functions to API endpoints.\n",
    " \n",
    "The full code we use to do this is available in the `server.py` file in the `examples/santacoder` folder on BlindBox's official GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mithril-security/blindbox\n",
    "!cd examples/santacoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key sections in this code:\n",
    "\n",
    "### Initial set-up\n",
    "\n",
    "Firstly, we load the santacoder model from Hugging Face, and initialize our API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model_name = \"bigcode/santacoder\"\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "device = \"cpu\" # cuda for GPU usage or cpu for CPU usage\n",
    "\n",
    "# get model and call eval\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(device).eval()\n",
    "\n",
    "# instantiate API object\n",
    "app = FastAPI()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an endpoint\n",
    "\n",
    "Secondly, we create a `/generate` POST endpoint on our FastAPI application object. The user will be able to send their prompt to this endpoint and get back the model's generated code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# define GenerateRequest input object\n",
    "class GenerateRequest(BaseModel):\n",
    "    input_text: str\n",
    "    max_new_tokens: int = 128\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "def generate(req: GenerateRequest):\n",
    "\n",
    "    # We go from string to token lists\n",
    "    inputs = tokenizer.encode(req.input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # query model with inputs\n",
    "    outputs = model.generate(inputs)\n",
    "    \n",
    "    # Convert tokens back to a string\n",
    "    text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    return {\"text\": text}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching our server\n",
    "\n",
    "Finally, we deploys our API on a python ASGI `uvicorn` server (an asynchronous web server) on `port 80`. It is essential to use **port 80** as BlindBox will need to be able to communicate with our application on this port!\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=80)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, we packaged the Whisper model as an API by doing the following:\n",
    "\n",
    "+ Creating an API app object that \"configures\" the `uvicorn` server by providing handlers for specific endpoints\n",
    "\n",
    "+ Creating a `generate` endpoint which in turn queries the santacoder model.\n",
    "\n",
    "+ Deploy our API on our server on `port 80`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging our application in a Docker image\n",
    "________________________________\n",
    "\n",
    "Once we had created out Whisper API, all that was left to do was create a **Docker image** for our application that could then be deployed in BlindBox. Let's take a look at the Dockerfile we used to do this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```docker\n",
    "FROM python:3.10.10-bullseye as base\n",
    "\n",
    "# install dependencies\n",
    "RUN pip install \\\n",
    "    torch==1.13.1 \\\n",
    "    transformers==4.26.1 \\\n",
    "    fastapi==0.95.0 \\\n",
    "    python-multipart==0.0.6 \\\n",
    "    uvicorn==0.21.1 \\\n",
    "    pydantic==1.10.7 \\\n",
    "    intel_extension_for_pytorch==1.13.100 \\\n",
    "    --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# copy our app code to container\n",
    "COPY ./server.py ./\n",
    "\n",
    "# signal that our application runs on port 80\n",
    "EXPOSE 80\n",
    "\n",
    "# run our app server\n",
    "CMD python ./server.py\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Same as for the application code, this file can be viewed in the `examples/santacoder` folder on the official BlindBox GitHub repository.\n",
    "\n",
    "There are no complex requirements for the Docker image, but it is recommended to `EXPOSE` port 80 to signal that the application will be running on port 80 within our BlindBox."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying our application\n",
    "\n",
    "We explain how to install everything you need to deploy applications with BlindBox in our [Quick Tour](../../docs/docs/getting-started/quick-tour.ipynb). \n",
    "\n",
    ">Please refer to the quick tour for more details about this or the following steps!\n",
    "\n",
    "Let's take a brief look at the code we used to deploy our BlindBox on a system which already had all the pre-requisites set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and navigate to santacoder folder\n",
    "!git clone https://github.com/mithril-security/blindbox\n",
    "!cd examples/santacoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the blindbox\n",
    "!blindbox --platform azure-sev init\n",
    "\n",
    "# build santacoder application assigning it the tag \"myimage\"\n",
    "!docker build -t myimage .\n",
    "\n",
    "# deploy our image inside Confidential VM using BlindBox\n",
    "!blindbox deploy myimage-blindbox:v1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying our application\n",
    "\n",
    "We were then able to query our santacoder application within our Confidential BlindBox. The code we send to it is protected from outside access at every step of its journey!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install blindbox\n",
    "!pip install blindbox\n",
    "\n",
    "# import requests module\n",
    "from blindbox  import requests\n",
    "\n",
    "CONFIDENTIAL_VM_IP_ADDRESS = \"127.0.0.1:80\" # replace with your VM IP address and port\n",
    "\n",
    "# we query our application, sending it a snippet of code to be completed by the model\n",
    "res = requests.post(url=f\"http://{CONFIDENTIAL_VM_IP_ADDRESS}/generate\", json={\"input_text\": \"def print_hello_world():\"})\n",
    "\n",
    "# display result\n",
    "print(res.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back our completed code block generated by the model!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "_______________________\n",
    " \n",
    "In this tutorial, we've seen how we can:\n",
    "+ Create a **BlindBox-compatible application**\n",
    "+ Create an **application image**, ready to be built and deployed on BlindBox!\n",
    "\n",
    "For more details on the deployment process, see our [quick tour](../../docs/docs/quick-tour)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "blindbox-preview-7Yaoi9am-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
